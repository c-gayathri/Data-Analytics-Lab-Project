{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and assess the accuracy of the model\n",
    "def get_accuracy(y_train, y_pred, y_test, y_test_pred):\n",
    "    print(\"Train Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "    print(\"Train Confusion Matrix:\\n\", confusion_matrix(y_train, y_pred))\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "    print(\"F1 Train:\", f1_score(y_train, y_pred, average='weighted'))\n",
    "    print(\"F1 Test:\", f1_score(y_test, y_test_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the public data. Transposes each image\n",
    "# and then saves the same\n",
    "def get_public_test():\n",
    "    df = pd.read_csv('public_test.csv', header=None)\n",
    "    X_pt = df.to_numpy().T\n",
    "    X_public_test = np.zeros(X_pt.shape)\n",
    "    for i in tqdm(range(X_pt.shape[0])):\n",
    "        X_public_test[i,:] = X_pt[i,:].reshape(28,28).T.reshape(-1,)\n",
    "    \n",
    "    return X_public_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform interpolation on the training Dataset\n",
    "\n",
    "# Get the 3*3 array of all values around any image\n",
    "neighbors = [[i,j] for i in range(-1,2) for j in range(-1,2)]\n",
    "neighbors = np.array(neighbors)\n",
    "\n",
    "def interpolate_data_threshold(df, threshold=20):\n",
    "    \"\"\"\n",
    "    The interpolate_data_threshold function performs thresholded\n",
    "    interpolation on the Training Dataset.\n",
    "    Input:\n",
    "        - df: Image as a DataFrame\n",
    "        - threshold: Threshold value below which \n",
    "                      the average is set to 0\n",
    "    Output:\n",
    "        - df_threshold: Interpolated image.\n",
    "    \"\"\"\n",
    "\n",
    "    data = df.to_numpy()\n",
    "    positions = np.argwhere(np.isnan(data))\n",
    "    num_loops = 0\n",
    "    while positions.size >= 0 and num_loops < 15:\n",
    "        num_loops += 1\n",
    "        for pos in positions:\n",
    "            consider = []\n",
    "            for neigh in neighbors:\n",
    "                summation = pos+neigh\n",
    "                if all(summation > 0) and all(summation < 27) and (not np.isnan(data[summation[0], summation[1]])):\n",
    "                    consider.append(data[summation[0]][summation[1]])\n",
    "            consider = np.array(consider)\n",
    "            if consider.size > 0:\n",
    "                data[pos[0]][pos[1]] = int(np.ceil(np.mean(consider)))*int(np.ceil(np.mean(consider)) > threshold)\n",
    "        positions = np.argwhere(np.isnan(data))\n",
    "    \n",
    "    if np.isnan(data[0][0]):\n",
    "        data[0][0] = int(np.ceil(np.mean(data[0:1][1], data[1][0])))\n",
    "    df_threshold = pd.DataFrame(data)\n",
    "    return df_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Character 0: 100%|██████████| 1000/1000 [00:16<00:00, 59.11it/s]\n",
      "Character 1: 100%|██████████| 1000/1000 [00:16<00:00, 59.92it/s]\n",
      "Character 2: 100%|██████████| 1000/1000 [00:16<00:00, 60.87it/s]\n",
      "Character 3: 100%|██████████| 1000/1000 [00:16<00:00, 61.42it/s]\n",
      "Character 4: 100%|██████████| 1000/1000 [00:15<00:00, 63.88it/s]\n",
      "Character 5: 100%|██████████| 1000/1000 [00:15<00:00, 64.96it/s]\n",
      "Character 6: 100%|██████████| 1000/1000 [00:15<00:00, 65.05it/s]\n",
      "Character 7: 100%|██████████| 1000/1000 [00:15<00:00, 65.04it/s]\n",
      "Character 8: 100%|██████████| 1000/1000 [00:15<00:00, 63.05it/s]\n",
      "Character 9: 100%|██████████| 1000/1000 [00:15<00:00, 64.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the interpolated thresholded images in X, y\n",
    "X = np.zeros((10000, 784))\n",
    "y = np.zeros((10000))\n",
    "\n",
    "for char in range(10):\n",
    "    # Access the data from the first 1000 files for each character\n",
    "    for i in tqdm(range(1,1001), desc=\"Character \"+str(char)):\n",
    "        fname = \"Training Dataset/character_\" + str(char) + \"/\" + str(i) + \".csv\"\n",
    "        df = pd.read_csv(fname, header=None)\n",
    "        df_threshold = interpolate_data_threshold(df, threshold=20)\n",
    "        df_threshold = df_threshold.astype(int)\n",
    "        image = df_threshold.to_numpy().reshape(-1,)\n",
    "        X[char*1000+i-1,:] = image\n",
    "        y[char*1000+i-1] = char\n",
    "\n",
    "# Create backup data\n",
    "X_backup = X.copy()\n",
    "y_backup = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 209558.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Using startified split to ensure equal class distribution\n",
    "stratSplit = StratifiedShuffleSplit(test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in stratSplit.split(X, y):\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "X_public_test = get_public_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 9, 7, 7, 8, 7, 3, 7, 9, 1, 9, 4, 8, 2, 5, 9, 9, 5, 0, 4, 7,\n",
       "       4, 9, 4, 3, 6, 6, 9, 9, 6, 7, 0, 5, 0, 7, 6, 1, 5, 9, 4, 4, 9, 9,\n",
       "       0, 1, 7, 9, 2, 2, 0, 5, 5, 5, 6, 1, 0, 3, 9, 7, 5, 3, 7, 6, 3, 6,\n",
       "       8, 4, 6, 8, 8, 1, 2, 7, 5, 7, 8, 7, 3, 6, 8, 4, 8, 3, 0, 8, 6, 7,\n",
       "       6, 0, 7, 0, 3, 0, 7, 5, 3, 0, 2, 7, 7, 6, 4, 9, 6, 4, 1, 3, 8, 8,\n",
       "       6, 6, 1, 0, 5, 2, 7, 6, 6, 2, 3, 3, 2, 4, 3, 2, 8, 3, 6, 0, 4, 4,\n",
       "       2, 7, 2, 4, 1, 4, 3, 2, 4, 9, 5, 5, 1, 9, 2, 2, 5, 0, 6, 5, 8, 4,\n",
       "       6, 0, 1, 2, 1, 5, 8, 7, 6, 6, 4, 8, 1, 5, 1, 1, 3, 2, 2, 9, 2, 0,\n",
       "       9, 0, 6, 1, 6, 1, 2, 4, 8, 9, 1, 2, 8, 4, 1, 2, 2, 8, 4, 7, 2, 9,\n",
       "       4, 0, 5, 9, 8, 6, 2, 8, 0, 7, 5, 9, 3, 9, 2, 9, 7, 4, 5, 9, 8, 7,\n",
       "       5, 0, 0, 5, 4, 4, 4, 5, 1, 0, 9, 9, 4, 6, 6, 8, 9, 3, 2, 8, 1, 1,\n",
       "       1, 7, 4, 8, 2, 9, 2, 5, 0, 0, 4, 3, 6, 3, 2, 7, 0, 7, 3, 8, 7, 5,\n",
       "       1, 5, 9, 8, 3, 3, 9, 3, 8, 5, 9, 1, 4, 1, 0, 6, 0, 0, 8, 5, 3, 4,\n",
       "       9, 2, 2, 7, 5, 8, 0, 3, 1, 1, 6, 6, 4, 3, 2, 4, 6, 4, 3, 2, 0, 6,\n",
       "       3, 3, 5, 0, 1, 1, 0, 1, 1, 8, 4, 8, 6, 8, 4, 2, 8, 3, 6, 2, 7, 4,\n",
       "       2, 5, 0, 2, 1, 2, 0, 3, 7, 2, 3, 8, 9, 7, 0, 2, 2, 5, 6, 3, 8, 3,\n",
       "       8, 9, 3, 2, 4, 1, 9, 0, 6, 4, 3, 5, 1, 2, 0, 8, 2, 2, 0, 9, 6, 9,\n",
       "       0, 0, 2, 4, 3, 9, 9, 5, 0, 8, 8, 4, 9, 9, 2, 1, 3, 0, 0, 1, 8, 5,\n",
       "       7, 3, 2, 6, 8, 5, 7, 0, 6, 9, 0, 1, 5, 9, 2, 2, 9, 2, 4, 5, 8, 4,\n",
       "       4, 7, 1, 9, 5, 1, 5, 8, 3, 7, 6, 6, 9, 1, 3, 7, 8, 0, 6, 6, 9, 4,\n",
       "       0, 3, 3, 1, 3, 1, 8, 9, 4, 5, 7, 2, 4, 4, 1, 6, 7, 3, 6, 2, 9, 1,\n",
       "       6, 9, 2, 5, 9, 7, 3, 0, 9, 9, 9, 1, 4, 4, 4, 7, 5, 7, 5, 1, 9, 0,\n",
       "       0, 7, 7, 6, 4, 7, 8, 2, 9, 5, 5, 9, 1, 9, 7, 3, 3, 8, 6, 1, 5, 5,\n",
       "       0, 9, 8, 0, 1, 3, 9, 2, 3, 8, 9, 8, 2, 8, 8, 3, 7, 8, 5, 7, 1, 1,\n",
       "       9, 7, 8, 5, 8, 9, 1, 4, 1, 0, 4, 5, 3, 0, 0, 2, 3, 8, 8, 4, 6, 9,\n",
       "       6, 3, 8, 7, 9, 6, 6, 2, 5, 0, 4, 9, 6, 9, 6, 1, 9, 2, 2, 4, 9, 2,\n",
       "       0, 7, 6, 7, 4, 0, 5, 3, 8, 3, 4, 6, 6, 2, 0, 0, 5, 7, 8, 8, 1, 1,\n",
       "       2, 4, 9, 7, 0, 5, 7, 9, 5, 1, 0, 2, 5, 1, 2, 4, 0, 9, 4, 4, 1, 0,\n",
       "       0, 1, 8, 4, 3, 1, 3, 1, 1, 4, 6, 8, 2, 8, 5, 2, 9, 8, 6, 3, 9, 9,\n",
       "       1, 5, 0, 3, 4, 0, 3, 6, 4, 9, 6, 3, 2, 4, 8, 7, 0, 0, 4, 9, 5, 5,\n",
       "       7, 9, 3, 2, 0, 9, 8, 5, 7, 7, 9, 9, 8, 5, 5, 8, 6, 3, 1, 0, 1, 1,\n",
       "       3, 5, 5, 9, 0, 9, 8, 7, 5, 3, 7, 5, 7, 0, 3, 7, 1, 3, 3, 4, 3, 8,\n",
       "       6, 8, 4, 6, 3, 7, 6, 1, 3, 4, 9, 7, 4, 4, 4, 1, 2, 4, 2, 2, 5, 8,\n",
       "       9, 5, 2, 7, 1, 3, 4, 1, 6, 3, 9, 9, 6, 3, 5, 0, 1, 2, 4, 1, 8, 4,\n",
       "       3, 1, 0, 2, 3, 8, 7, 6, 7, 5, 0, 3, 0, 2, 3, 6, 1, 2, 4, 2, 4, 1,\n",
       "       2, 1, 6, 7, 2, 7, 2, 1, 5, 7, 1, 9, 0, 4, 8, 2, 7, 0, 1, 6, 2, 3,\n",
       "       7, 7, 9, 4, 8, 8, 7, 2, 0, 8, 1, 0, 6, 1, 0, 5, 6, 3, 8, 0, 7, 0,\n",
       "       0, 1, 7, 2, 1, 8, 4, 1, 1, 5, 6, 5, 7, 7, 0, 9, 6, 3, 4, 1, 2, 8,\n",
       "       1, 3, 6, 6, 1, 3, 8, 2, 8, 2, 4, 3, 8, 0, 8, 3, 1, 9, 8, 9, 6, 6,\n",
       "       5, 6, 0, 5, 6, 0, 4, 7, 1, 3, 1, 2, 6, 4, 7, 4, 5, 6, 7, 4, 4, 7,\n",
       "       3, 7, 2, 8, 2, 0, 9, 5, 1, 7, 6, 0, 6, 3, 1, 5, 0, 7, 3, 6, 0, 8,\n",
       "       0, 8, 7, 6, 2, 0, 7, 6, 5, 4, 5, 5, 3, 5, 4, 7, 5, 8, 2, 1, 2, 9,\n",
       "       5, 5, 7, 5, 9, 4, 5, 7, 7, 3, 4, 0, 4, 5, 7, 7, 0, 6, 6, 1, 4, 6,\n",
       "       8, 5, 8, 9, 5, 1, 3, 5, 4, 8, 2, 7, 5, 6, 2, 8, 1, 8, 3, 5, 9, 7,\n",
       "       5, 2, 8, 6, 6, 0, 0, 4, 1, 5, 2, 1, 5, 9, 5, 5, 7, 6, 7, 7, 8, 7,\n",
       "       4, 5, 5, 1, 9, 8, 8, 0, 6, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regularisation Parameter C = 15\n",
    "# PCA - 75 Components followed by SVM\n",
    "# The parameters were obtained after performaing a GridSearchCV\n",
    "\n",
    "pipe = Pipeline([('pca', PCA(n_components=75, random_state=69)), ('svm', SVC(C=15, random_state=69))])\n",
    "pipe = pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_train)\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# get_accuracy(y_train, y_pred, y_test, y_test_pred)\n",
    "\n",
    "y_public_pred = pipe.predict(X_public_test).astype(int)\n",
    "y_public_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classifier = open(\"pipe.pickle\",\"wb\")\n",
    "pickle.dump(pipe, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_private_test():\n",
    "    df = pd.read_csv('private_test.csv', header=None)\n",
    "    X_pt = df.to_numpy().T\n",
    "    X_private_test = np.zeros(X_pt.shape)\n",
    "    for i in tqdm(range(X_pt.shape[0])):\n",
    "        X_private_test[i,:] = X_pt[i,:].reshape(28,28).T.reshape(-1,)\n",
    "    \n",
    "    return X_private_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_private_dataset():\n",
    "    pipe = pickle.load(open(\"pipe.pickle\", 'rb'))\n",
    "    X_private = get_private_test()\n",
    "    predicted_class = pipe.predict(X_private)\n",
    "    predicted_class = predicted_class.reshape(-1,1).astype(int)\n",
    "    \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combination \n",
    "# pca = PCA(n_components=75, random_state=69)\n",
    "# pca = pca.fit(X_train, y_train)\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "# X_public_test_pca = pca.transform(X_public_test)\n",
    "\n",
    "# y_pred_c = {}\n",
    "# y_test_pred_c = {}\n",
    "# clfs = {}\n",
    "# y_pub_pred_c = {}\n",
    "\n",
    "# # Constructing classifiers for every class and saving the classwise predictions in 10 arrays\n",
    "# for c in tqdm(range(10)):\n",
    "#     clfs[c] = SVC(C=15, random_state=69, probability=True)\n",
    "# #     clfs[c] = SVC(C=15, probability=True)\n",
    "#     y_c = (y_train == c).astype(int)\n",
    "#     clfs[c].fit(X_train_pca, y_c)\n",
    "#     y_pred_c[c] = clfs[c].predict(X_train_pca)\n",
    "#     y_test_pred_c[c] = clfs[c].predict(X_test_pca)\n",
    "#     y_pub_pred_c[c] = clfs[c].predict(X_public_test_pca)\n",
    "\n",
    "# # Classifier for a separate step\n",
    "# clf_l = KNeighborsClassifier(3)\n",
    "# clf_l.fit(X_train_pca, y_train)\n",
    "\n",
    "# # classify X_pca first based on clf_l.\n",
    "# # The ones with probability lower than 0.6 in clf_l are classified again, classwise, using the various clfs\n",
    "# def probwise_predict(clfs, X_pca, clf_l):\n",
    "#     y_pred_c = {}\n",
    "#     prob_c = {}\n",
    "\n",
    "#     # predict the labels and probabilities of classwise classifiers\n",
    "#     for c in range(10):\n",
    "#         y_pred_c[c] = clfs[c].predict(X_pca)\n",
    "#         prob = clfs[c].predict_proba(X_pca)\n",
    "#         prob_c[c] = prob.max(axis = 1)\n",
    "#     prob_c = pd.DataFrame(prob_c)\n",
    "\n",
    "#     prob_clf_c = clf_l.predict_proba(X_train_pca)\n",
    "#     eg = pd.DataFrame(prob_clf_c)\n",
    "#     # display(eg)\n",
    "#     prob_clf_c = clf_l.predict_proba(X_train_pca).max(axis = 1)\n",
    "#     # print(prob_clf_c)\n",
    "#     # print(prob_clf_c.shape)\n",
    "\n",
    "#     #create a boolean array that takes the value 1 where the classwise classification yields multiple or zero labels\n",
    "#     y_pred_c = pd.DataFrame(y_pred_c)\n",
    "#     summed = y_pred_c.sum(axis = 1)\n",
    "#     boo = (summed != 1).astype(int).to_numpy()\n",
    "#     # extract X and y values where boo = 1\n",
    "#     if 1 in boo:\n",
    "#         X_residue = X_pca[boo == 1]\n",
    "#         y_res = clf_l.predict(X_residue)\n",
    "\n",
    "#     y_pred = np.zeros(len(y_pred_c))\n",
    "\n",
    "#     count_p = 0\n",
    "#     count_r = 0\n",
    "\n",
    "#     # First, for each datapoint, check if classwise classification yielded a single answer.\n",
    "#     # If yes, assign the predicted label\n",
    "#     # If not, check the classification probabily yielded by clf_l.\n",
    "#     # If greater than 0.6, assign the label predicted by clf_l\n",
    "#     # If lesser than 0.6, assign the label with the maximum probability amongst the classwise predictors\n",
    "#     for i,v in enumerate(y_pred_c.values):\n",
    "#         if boo[i]:\n",
    "#             p = prob_clf_c[i]\n",
    "#             if p > 0.6:\n",
    "#                 y_pred[i] = clf_l.predict(X_pca[i].reshape(1, -1))\n",
    "#                 count_r += 1\n",
    "#             else:\n",
    "#                 row = prob_c.iloc[i]\n",
    "#                 p_max = row.max()\n",
    "#                 pos = np.where(row == p_max)[0][0]\n",
    "#                 y_pred[i] = pos\n",
    "#                 count_p += 1\n",
    "#         else:\n",
    "#             y_pred[i] = np.where(v == 1)[0][0]\n",
    "\n",
    "#     return(y_pred, boo, count_p, count_r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# y_test_prob_pred, boo, count_p, count_r = probwise_predict(clfs, X_test_pca, clf_l)\n",
    "# y_public_test_prob_pred, boo_public, count_p_public, count_r_public = probwise_predict(clfs, X_public_test_pca, clf_l)\n",
    "\n",
    "# s8 = np.array([6, 4, 9, 7, 7, 8, 7, 3, 7, 9, 1, 9, 4, 8, 2, 5, 9, 9, 5, 0, 4, 7, 4, 9, 4, 3, 6, 6, 9, 9, 6, 7, 0, 5, 0, 7, 6, 1, 5, 9, 4, 4, 9, 9, 0, 1, 7, 9, 2, 2, 0, 5, 5, 5, 6, 1, 0, 3, 9, 7, 5, 3, 7, 6, 3, 6, 8, 4, 6, 8, 8, 1, 2, 7, 5, 7, 8, 7, 3, 6, 8, 4, 8, 3, 0, 8, 6, 7, 6, 0, 7, 0, 3, 0, 7, 5, 3, 0, 2, 7, 7, 6, 4, 9, 6, 4, 1, 3, 8, 8, 6, 6, 1, 0, 5, 2, 7, 6, 6, 2, 3, 3, 2, 4, 3, 2, 8, 3, 6, 0, 4, 4, 2, 7, 2, 4, 1, 4, 3, 2, 4, 9, 5, 5, 1, 9, 2, 2, 5, 0, 6, 5, 8, 4, 6, 0, 1, 2, 1, 5, 8, 7, 6, 6, 4, 8, 1, 5, 1, 1, 3, 2, 2, 9, 2, 0, 9, 0, 6, 1, 6, 1, 2, 4, 8, 9, 1, 2, 8, 4, 1, 2, 2, 8, 4, 7, 2, 9, 4, 0, 5, 9, 8, 6, 2, 8, 0, 7, 5, 9, 3, 9, 2, 9, 7, 4, 5, 9, 8, 7, 5, 0, 0, 5, 4, 4, 4, 5, 1, 0, 9, 9, 4, 6, 6, 8, 9, 3, 2, 8, 1, 1, 1, 7, 4, 8, 2, 9, 2, 5, 0, 0, 4, 3, 6, 3, 2, 7, 0, 7, 3, 8, 7, 5, 1, 5, 9, 8, 3, 3, 9, 3, 8, 5, 9, 1, 4, 1, 0, 6, 0, 0, 8, 5, 3, 4, 9, 2, 2, 7, 5, 8, 0, 3, 1, 1, 6, 6, 4, 3, 2, 4, 6, 4, 3, 2, 0, 6, 3, 3, 5, 0, 1, 1, 0, 1, 1, 8, 4, 8, 6, 8, 4, 2, 8, 3, 6, 2, 7, 4, 2, 5, 0, 2, 1, 2, 0, 3, 7, 2, 3, 8, 9, 7, 0, 2, 2, 5, 6, 3, 8, 3, 8, 9, 3, 2, 4, 1, 9, 0, 6, 4, 3, 5, 1, 2, 0, 8, 2, 2, 0, 9, 6, 9, 0, 0, 2, 4, 3, 9, 9, 5, 0, 8, 8, 4, 9, 9, 2, 1, 3, 0, 0, 1, 8, 5, 7, 3, 2, 6, 8, 5, 7, 0, 6, 9, 0, 1, 5, 9, 2, 2, 9, 2, 4, 5, 8, 4, 4, 7, 1, 9, 5, 1, 5, 8, 3, 7, 6, 6, 9, 1, 3, 7, 8, 0, 6, 6, 9, 4, 0, 3, 3, 1, 3, 1, 8, 9, 4, 5, 7, 2, 4, 4, 1, 6, 7, 3, 6, 2, 9, 1, 6, 9, 2, 5, 9, 7, 3, 0, 9, 9, 9, 1, 4, 4, 4, 7, 5, 7, 5, 1, 9, 0, 0, 7, 7, 6, 4, 7, 8, 2, 9, 5, 5, 9, 1, 9, 7, 3, 3, 8, 6, 1, 5, 5, 0, 9, 8, 0, 1, 3, 9, 2, 3, 8, 9, 8, 2, 8, 8, 3, 7, 8, 5, 7, 1, 1, 9, 7, 8, 5, 8, 9, 1, 4, 1, 0, 4, 5, 3, 0, 0, 2, 3, 8, 8, 4, 6, 9, 6, 3, 8, 7, 9, 6, 6, 2, 5, 0, 4, 9, 6, 9, 6, 1, 9, 2, 2, 4, 9, 2, 0, 7, 6, 7, 4, 0, 5, 3, 8, 3, 4, 6, 6, 2, 0, 0, 5, 7, 8, 8, 1, 1, 2, 4, 9, 7, 0, 5, 7, 9, 5, 1, 0, 2, 5, 1, 2, 4, 0, 9, 4, 4, 1, 0, 0, 1, 8, 4, 3, 1, 3, 1, 1, 4, 6, 8, 2, 8, 5, 2, 9, 8, 6, 3, 9, 9, 1, 5, 0, 3, 4, 0, 3, 6, 4, 9, 6, 3, 2, 4, 8, 7, 0, 0, 4, 9, 5, 5, 7, 9, 3, 2, 0, 9, 8, 5, 7, 7, 9, 9, 8, 5, 5, 8, 6, 3, 1, 0, 1, 1, 3, 5, 5, 9, 0, 9, 8, 7, 5, 3, 7, 5, 7, 0, 3, 7, 1, 3, 3, 4, 3, 8, 6, 8, 4, 6, 3, 7, 6, 1, 3, 4, 9, 7, 4, 4, 4, 1, 2, 4, 2, 2, 5, 8, 9, 5, 2, 7, 1, 3, 4, 1, 6, 3, 9, 9, 6, 3, 5, 0, 1, 2, 4, 1, 8, 4, 3, 1, 0, 2, 3, 8, 7, 6, 7, 5, 0, 3, 0, 2, 3, 6, 1, 2, 4, 2, 4, 1, 2, 1, 6, 7, 2, 7, 2, 1, 5, 7, 1, 9, 0, 4, 8, 2, 7, 0, 1, 6, 2, 3, 7, 7, 9, 4, 8, 8, 7, 2, 0, 8, 1, 0, 6, 1, 0, 5, 6, 3, 8, 0, 7, 0, 0, 1, 7, 2, 1, 8, 4, 1, 1, 5, 6, 5, 7, 7, 0, 9, 6, 3, 4, 1, 2, 8, 1, 3, 6, 6, 1, 3, 8, 2, 8, 2, 4, 3, 8, 0, 8, 3, 1, 9, 8, 9, 6, 6, 5, 6, 0, 5, 6, 0, 4, 7, 1, 3, 1, 2, 6, 4, 7, 4, 5, 6, 7, 4, 4, 7, 3, 7, 2, 8, 2, 0, 9, 5, 1, 7, 6, 0, 6, 3, 1, 5, 0, 7, 3, 6, 0, 8, 0, 8, 7, 6, 2, 0, 7, 6, 5, 4, 5, 5, 3, 5, 4, 7, 5, 8, 2, 1, 2, 9, 5, 5, 7, 5, 9, 4, 5, 7, 7, 3, 4, 0, 4, 5, 7, 7, 0, 6, 6, 1, 4, 6, 8, 5, 8, 9, 5, 1, 3, 5, 4, 8, 2, 7, 5, 6, 2, 8, 1, 8, 3, 5, 9, 7, 5, 2, 8, 6, 6, 0, 0, 4, 1, 5, 2, 1, 5, 9, 5, 5, 7, 6, 7, 7, 8, 7, 4, 5, 5, 1, 9, 8, 8, 0, 6, 4])\n",
    "# pos = np.where(y_public_test_prob_pred!=s8)\n",
    "# print(\"Mismatch:\", pos[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape\n",
    "# x = X_train.T\n",
    "# x = x-np.mean(x, axis=1).reshape(-1,1)\n",
    "# N = x.shape[1]\n",
    "# S = (1/N)*(x@x.T)\n",
    "# w, v = np.linalg.eig(S)\n",
    "# plt.plot(w[:80], '.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "\n",
    "# # clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
    "# clf1 = KNeighborsClassifier(weights='distance')\n",
    "# clf2 = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
    "# clf3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "# clf4 = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "# clf5 = SVC(C=15, random_state=1)\n",
    "# clf7 = SVC(C=10, random_state=1)\n",
    "# kernel = RationalQuadratic(length_scale=1000, alpha=20)\n",
    "# gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(X_train_pca, y_train)\n",
    "\n",
    "# eclf1 = VotingClassifier(estimators=[('knn', clf1), ('knn2', clf2), ('rf', clf3), \n",
    "#                                      ('rf2', clf4), ('svm1', clf5), ('gpc', gpc), \n",
    "#                                      ('svm3', clf7)], voting='hard')\n",
    "# eclf1 = eclf1.fit(X_train_pca, y_train)\n",
    "\n",
    "# y_pred = eclf1.predict(X_train_pca)\n",
    "# y_test_pred = eclf1.predict(X_test_pca)\n",
    "\n",
    "# get_accuracy(y_train, y_pred, y_test, y_test_pred)\n",
    "\n",
    "# y_public_pred = eclf1.predict(X_public_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# pca = PCA(n_components=75)\n",
    "# pca = pca.fit(X_train, y_train)\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# clf = BaggingClassifier(base_estimator=SVC(C=15), n_estimators=200, random_state=0, bootstrap=False)\n",
    "# clf.fit(X_train_pca, y_train)\n",
    "# y_pred = clf.predict(X_train_pca)\n",
    "# y_test_pred = clf.predict(X_test_pca)\n",
    "\n",
    "# get_accuracy(y_train, y_pred, y_test, y_test_pred)\n",
    "\n",
    "# # F1 Train: 0.9896183054246059\n",
    "# # F1 Test: 0.9613735905734541\n",
    "# # 100, bootstrap=False\n",
    "# # F1 Train: 0.9892414065774306\n",
    "# # F1 Test: 0.9594006816763111\n",
    "# # 100, bootstrap=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commented Out. Because it takes long time to run\n",
    "# # SVM with RBF\n",
    "# clf = SVC()\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred_SVM = clf.predict(X_train)\n",
    "# y_test_pred_SVM = clf.predict(X_test)\n",
    "\n",
    "# get_accuracy(y_train, y_pred_SVM, y_test, y_test_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Classifier\n",
    "# model = RandomForestClassifier(max_depth=11, criterion='entropy')\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred_RF = model.predict(X_train)\n",
    "# y_test_pred_RF = model.predict(X_test)\n",
    "\n",
    "# get_accuracy(y_train, y_pred_RF, y_test, y_test_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision Tree Classifier\n",
    "# clf = tree.DecisionTreeClassifier(min_samples_split=5)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred_DT = clf.predict(X_train)\n",
    "# y_test_pred_DT = clf.predict(X_test)\n",
    "\n",
    "# get_accuracy(y_train, y_pred_DT, y_test, y_test_pred_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM with Poly\n",
    "# clf = SVC(kernel='poly', degree=9)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred_SVMP = clf.predict(X_train)\n",
    "# y_test_pred_SVMP = clf.predict(X_test)\n",
    "\n",
    "# get_accuracy(y_train, y_pred_SVMP, y_test, y_test_pred_SVMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM with RBF; gamma-auto\n",
    "# clf = SVC(gamma='auto')\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred_SVMA = clf.predict(X_train)\n",
    "# y_test_pred_SVMA = clf.predict(X_test)\n",
    "\n",
    "# get_accuracy(y_train, y_pred_SVMA, y_test, y_test_pred_SVMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM with Sigmoid\n",
    "# clf = SVC(kernel='sigmoid')\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred_LO = clf.predict(X_train)\n",
    "# y_test_pred_LO = clf.predict(X_test)\n",
    "\n",
    "# get_accuracy(y_train, y_pred_LO, y_test, y_test_pred_LO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Commented Out. Because it takes long time to run\n",
    "# # Submission 3\n",
    "# # Apply PCA and then SVM\n",
    "# num_components = 150\n",
    "# pca = PCA(n_components=num_components)\n",
    "# pca.fit(X_train)\n",
    "\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# clf = SVC()\n",
    "# clf = clf.fit(X_train_pca, y_train)\n",
    "\n",
    "# y_pred_PCA = clf.predict(X_train_pca)\n",
    "# y_test_pred_PCA = clf.predict(X_test_pca)\n",
    "\n",
    "# get_accuracy(y_train, y_pred_PCA, y_test, y_test_pred_PCA)\n",
    "\n",
    "# df_test = pd.read_csv(\"processed_test.csv\")\n",
    "# X_public_test = df_test.to_numpy().T\n",
    "# X_public_test_pca = pca.transform(X_public_test)\n",
    "# y_public_pred = clf.predict(X_public_test_pca)\n",
    "# y_public_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Submission 5\n",
    "# # PCA - 75 Components followed by SVM\n",
    "\n",
    "# num_components = 75\n",
    "# pca = PCA(n_components=num_components)\n",
    "# pca.fit(X_train)\n",
    "\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# clf = SVC()\n",
    "# clf = clf.fit(X_train_pca, y_train)\n",
    "\n",
    "# y_pred_PCA = clf.predict(X_train_pca)\n",
    "# y_test_pred_PCA = clf.predict(X_test_pca)\n",
    "\n",
    "# get_accuracy(y_train, y_pred_PCA, y_test, y_test_pred_PCA)\n",
    "\n",
    "# df_test = pd.read_csv(\"processed_test.csv\")\n",
    "# X_public_test = df_test.to_numpy().T\n",
    "# X_public_test_pca = pca.transform(X_public_test)\n",
    "# y_public_pred = clf.predict(X_public_test_pca)\n",
    "# y_public_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalising database\n",
    "# X = np.load('interpolated_traindata.npz')[\"X_train\"].T.astype(int)\n",
    "# y = np.load('interpolated_traindata.npz')[\"y_train\"].reshape(-1,).astype(int)\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# stratSplit = StratifiedShuffleSplit(test_size=0.2, random_state=42)\n",
    "\n",
    "# for train_idx, test_idx in stratSplit.split(X, y):\n",
    "#     X_train = X[train_idx]\n",
    "#     y_train = y[train_idx]\n",
    "#     X_test = X[test_idx]\n",
    "#     y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Next Attempt - LDA\n",
    "# clf = LinearDiscriminantAnalysis()\n",
    "# clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = clf.predict(X_train)\n",
    "# y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# df_test = pd.read_csv(\"processed_test.csv\")\n",
    "# X_public_test = df_test.to_numpy().T\n",
    "# y_public_pred = clf.predict(X_public_test)\n",
    "# y_public_pred\n",
    "\n",
    "# pos = np.where((s5!=y_public_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_c = {}\n",
    "# y_test_pred_c = {}\n",
    "# y_public_test_pred_c = {}\n",
    "\n",
    "# for c in tqdm(range(50)):\n",
    "#     pca = PCA(n_components=75, random_state=10*c)\n",
    "#     svm = SVC(C=15, random_state=10*c, probability=True)\n",
    "#     pipe = Pipeline(steps=[('pca', pca), ('svm', svm)])\n",
    "\n",
    "#     pipe.fit(X_train, y_train)\n",
    "#     y_pred_c[c] = pipe.predict(X_train)\n",
    "#     y_test_pred_c[c] = pipe.predict(X_test)\n",
    "#     y_public_test_pred_c[c] = pipe.predict(X_public_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attempt - HOG transformation\n",
    "# pca = PCA(random_state=10)\n",
    "# # set the tolerance to a large value to make the example faster\n",
    "# # 3\n",
    "# svm = SVC(random_state=10)\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('svm', svm)])\n",
    "\n",
    "# n_components = list(range(10, 50, 2))\n",
    "# n_components.append('mle')\n",
    "\n",
    "# # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "# param_grid = {\n",
    "#     'svm__C': [0, 1, 10, 15, 30, 50],\n",
    "#     'pca__n_components': n_components \n",
    "# }\n",
    "\n",
    "# search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "# search = search.fit(X_train_hog, y_train)\n",
    "# print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "# print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attempt scaling\n",
    "# X_train_scale = X_train/255\n",
    "# X_test_scale = X_test/255\n",
    "# X_public_test_scale = X_public_test/255\n",
    "\n",
    "# pca = PCA(random_state=10)\n",
    "# # set the tolerance to a large value to make the example faster\n",
    "# svm = SVC(random_state=10)\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('svm', svm)])\n",
    "\n",
    "# n_components = list(range(10, 50, 2))\n",
    "# n_components.append('mle')\n",
    "\n",
    "# # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "# param_grid = {\n",
    "#     'svm__C': [0, 1, 10, 15, 30, 50],\n",
    "#     'pca__n_components': n_components,\n",
    "#     'svm__gamma': ['auto', 0.001, 0.01, 0.1]\n",
    "# }\n",
    "\n",
    "# search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "# search = search.fit(X_train_scale, y_train)\n",
    "# print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "# print(search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
